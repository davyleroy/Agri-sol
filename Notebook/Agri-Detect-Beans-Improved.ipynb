{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports\n",
    "This block imports all the necessary libraries for the project. We'll need `tensorflow` and `keras` for building and training the model, `numpy` for numerical operations, `matplotlib` for plotting, and `sklearn` for evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(f'TensorFlow Version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preparation\n",
    "Here, we define the path to our dataset and the class labels. We'll then load all images, resize them to a uniform size, and convert them to NumPy arrays. The labels are also prepared and the data is split into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = r'C:\\Users\\mbuto\\Agri-sol\\Dataset\\Bean_Dataset'\n",
    "CLASSES = ['angular_leaf_spot', 'bean_rust', 'healthy']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "IMAGE_SIZE = (224, 224) # VGG16 was trained on 224x224 images\n",
    "\n",
    "def load_data(dataset_path, classes, image_size):\n",
    "    image_list, label_list = [], []\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        print(f'Loading images from: {class_path}')\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            try:\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Matplotlib uses RGB\n",
    "                    image = cv2.resize(image, image_size)\n",
    "                    image_list.append(img_to_array(image))\n",
    "                    label_list.append(i)\n",
    "            except Exception as e:\n",
    "                print(f'Error loading image {image_path}: {e}')\n",
    "    return np.array(image_list), np.array(label_list)\n",
    "\n",
    "X, y = load_data(DATASET_PATH, CLASSES, IMAGE_SIZE)\n",
    "print(f'\nTotal images loaded: {len(X)}')\n",
    "print(f'Class distribution: {Counter(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Split Data and Preprocess\n",
    "We normalize the pixel values to be between 0 and 1 and perform one-hot encoding on the labels. The data is split into 70% for training, 15% for validation, and 15% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "X = X.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_cat = to_categorical(y, NUM_CLASSES)\n",
    "\n",
    "# First split: 70% train, 30% temp (val + test)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(X, y_cat, test_size=0.3, random_state=42, stratify=y_cat)\n",
    "\n",
    "# Second split: 15% validation, 15% test from the temp set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f'Training data shape: {x_train.shape}')\n",
    "print(f'Validation data shape: {x_val.shape}')\n",
    "print(f'Testing data shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Augmentation\n",
    "To prevent overfitting and make the model more robust, we apply data augmentation to the training images. This creates modified versions of the images with random rotations, shifts, zooms, and flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Building (Transfer Learning)\n",
    "We use VGG16, a model pre-trained on the large ImageNet dataset, as our base. We freeze its layers to leverage its learned features and add our own custom classifier on top. This approach is powerful and efficient for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 base model without the top classifier\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Freeze the convolutional base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create the new model on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x) # Regularization\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training\n",
    "Now we train the model. We use callbacks to save the best version of the model (`ModelCheckpoint`), stop training early if performance doesn't improve (`EarlyStopping`), and reduce the learning rate on a plateau (`ReduceLROnPlateau`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1),\n",
    "    ModelCheckpoint(filepath='bean_disease_model_best.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n",
    "After training, we evaluate the model's performance. We plot the accuracy and loss curves to visualize the training process and then generate a classification report and confusion matrix to see how well the model performs on each class in the test set. This helps ensure the model is unbiased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='train_accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='train_loss')\n",
    "    ax2.plot(history.history['val_loss'], label='val_loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate on test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\nTest Accuracy: {accuracy*100:.2f}%')\n",
    "print(f'Test Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Classification Report and Confusion Matrix\n",
    "This provides a detailed breakdown of the model's predictions, showing precision, recall, and F1-score for each class. The confusion matrix visualizes where the model is making correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=CLASSES))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}